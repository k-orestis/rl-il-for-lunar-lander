{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "CUDA_DEVICE = torch.device('cuda')\n",
    "CPU_DEVICE = torch.device('cpu')\n",
    "DEVICE = CUDA_DEVICE if USE_GPU else CPU_DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Learning \n",
    "Train an agent with deep q or double deep q learning algorithm \\\n",
    "Watch a pretrained agent solving LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[251.33790420922693, 271.5718362863971, 264.98726907083756, 273.66507730860036]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dqAgent import DQAgent\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "ag_q = DQAgent(env, double_qn=False, device = DEVICE)\n",
    "#ag.train()\n",
    "ag_q.load_models('dq1')\n",
    "ag_q.visualize(ep = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization\n",
    "Train an agent with ppo algorithm, an algorithm besed on policy gradient and actor-ctitic paradigm \\\n",
    "Watch a pretrained agent solving LunarLander as good as DQAgent most of the times \\\n",
    "Now for training we need several environment in witch the agent interacts simultaneously \\\n",
    "An important hyperparameter is the rollout step that it is set by default as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[267.2439062563842, 253.55319460616138, 290.63272832621664]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ppoAgent import PPOAgent\n",
    "num_envs = 16\n",
    "env = [gym.make(\"LunarLander-v2\") for _ in range(num_envs)]\n",
    "ag_ppo = PPOAgent(env, num_env=num_envs, device=DEVICE)\n",
    "#ag_ppo.train()\n",
    "ag_ppo.load_models('ppo')\n",
    "ag_ppo.visualize(ep = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning \n",
    "For the following two algorithms we are going to need demonstrations of optimal interaction with the environment. \\\n",
    "For that we use collect data from the interaction of a pretrained RL Agent like DQAGent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = ag_q.get_experiences(num_exp=10*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioural Cloning\n",
    "We will train an agent to imitate the actions of an expert \\\n",
    "We collect touples of states and actions of the interaction of an rl agent like DQAGent and perform supervised learning on that dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-330.5044019014285,\n",
       " -263.29283521425646,\n",
       " -316.69186146196773,\n",
       " -259.34746298495253]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bcAgent import BCAgent\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "ag_bc = BCAgent(env, experience=ex, device=DEVICE)\n",
    "#ag_bc.train()\n",
    "ag_bc.load_models('bc6', 'cpu')\n",
    "ag_bc.visualize(ep = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqAgent import DQAgent\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "ag_dq = DQAgent(env, layers = [128,32], eps = 1, double_qn=True)\n",
    "#ag_q.load_models('dq1')\n",
    "ex = ag_dq.get_experiences(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adverarial Imitation Learning\n",
    "We will leverage the policy gradient paradigm and adversarial training techniques for imitation learning \\\n",
    "We use a PPOAgent as a generator that ouputs actions given states and a simple MLP classifier as a discriminator. The descriminator aims to distinguish between the state-action pairs provided by the generator and state-actions pairs provided by the expert. \\\n",
    "PPOAgent uses discriminator's output as reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gailAgent import GAILAgent\n",
    "num_envs = 16\n",
    "env = [gym.make(\"LunarLander-v2\") for _ in range(num_envs)]\n",
    "ag_gail = GAILAgent(env, expert_demos=ex, rollout_steps=64, device=DEVICE)\n",
    "#ag_gail.train()\n",
    "ag_gail.load_models('gail', 'cpu')\n",
    "ag_gail.visualize(ep = 4)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Conditioned Supervised Learning\n",
    "For this imitation learning algorithm no expert demonstrations are needed. The agent learns a goal conditioned policy by exploiting its own interactions with the environment. The algorithm main idea is that even if a trajectory is not optimal it is optimal for reaching the final state of the trajectory begining from the initial state of the trajectory \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the agent manages to reach its goal -> getting to point (0, 0) of the environment, while both rocket feet touch the moon. However, because it never sees the reward function it doens't learn to land the rocket smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orestis/Desktop/imitation learning/rl-il-for-lunar-lander/gcslAgent.py:160: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  obs = torch.tensor(obs, dtype=torch.float32, device = self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final state:  [ 0.05221357 -0.04342753  1.          1.        ]\n",
      "final state:  [-0.05183087 -0.04321696  1.          1.        ]\n",
      "final state:  [ 0.09947443 -0.01601993  0.          1.        ]\n",
      "final state:  [ 0.00568418 -0.03415377  1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "from gcslAgent import GCSLAgent\n",
    "env = gym.make('LunarLander-v2')\n",
    "ag_gcsl = GCSLAgent(env)\n",
    "ag_gcsl.load_models('gcsl')\n",
    "ag_gcsl.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
